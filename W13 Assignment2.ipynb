{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX9TAe2x3ETj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how\n",
        "can they be mitigated?\n",
        "\n",
        "Overfitting\n",
        "Overfitting occurs when our machine learning model tries to cover all the data points or more than the required data points present in the given dataset. Because of this, the model starts caching noise and inaccurate values present in the dataset, and all these factors reduce the efficiency and accuracy of the model. The overfitted model has low bias and high variance.\n",
        "The chances of occurrence of overfitting increase as much we provide training to our model. It means the more we train our model, the more chances of occurring the overfitted model.\n",
        "Overfitting is the main problem that occurs in supervised learning.\n",
        "How to avoid the Overfitting in Model\n",
        "Both overfitting and underfitting cause the degraded performance of the machine learning model. But the main cause is overfitting, so there are some ways by which we can reduce the occurrence of overfitting in our model.\n",
        "\n",
        "Cross-Validation\n",
        "Training with more data\n",
        "Removing features\n",
        "Early stopping the training\n",
        "Regularization\n",
        "Ensembling\n",
        "\n",
        "Underfitting\n",
        "Underfitting occurs when our machine learning model is not able to capture the underlying trend of the data. To avoid the overfitting in the model, the fed of training data can be stopped at an early stage, due to which the model may not learn enough from the training data. As a result, it may fail to find the best fit of the dominant trend in the data.\n",
        "\n",
        "In the case of underfitting, the model is not able to learn enough from the training data, and hence it reduces the accuracy and produces unreliable predictions.\n",
        "An underfitted model has high bias and low variance.\n",
        "\n",
        "How to avoid underfitting:\n",
        "By increasing the training time of the model.\n",
        "By increasing the number of features.\n",
        "Goodness of Fit\n",
        "The \"Goodness of fit\" term is taken from the statistics, and the goal of the machine learning models to achieve the goodness of fit. In statistics modeling, it defines how closely the result or predicted values match the true values of the dataset.\n",
        "\n",
        "The model with a good fit is between the underfitted and overfitted model, and ideally, it makes predictions with 0 errors, but in practice, it is difficult to achieve it.\n",
        "\n",
        "As when we train our model for a time, the errors in the training data go down, and the same happens with test data. But if we train the model for a long duration, then the performance of the model may decrease due to the overfitting, as the model also learn the noise present in the dataset. The errors in the test dataset start increasing, so the point, just before the raising of errors, is the good point, and we can stop here for achieving a good model."
      ],
      "metadata": {
        "id": "eq31QxlW3FDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: How can we reduce overfitting? Explain in brief.\n",
        "\n",
        "Techniques to Reduce Overfitting\n",
        "1.Improving the quality of training data reduces overfitting by focusing on meaningful patterns, mitigate the risk of fitting the noise or irrelevant features.\n",
        "2. Increase the training data can improve the model’s ability to generalize to unseen data and reduce the likelihood of overfitting.\n",
        "3. Reduce model complexity.\n",
        "4. Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
        "5.Ridge Regularization and Lasso Regularization.\n",
        "6. Use dropout for neural networks to tackle overfitting.\n",
        "\n",
        "Good Fit in a Statistical Model\n",
        "Ideally, the case when the model makes the predictions with 0 error, is said to have a good fit on the data. This situation is achievable at a spot between overfitting and underfitting. In order to understand it, we will have to look at the performance of our model with the passage of time, while it is learning from the training dataset.\n",
        "With the passage of time, our model will keep on learning, and thus the error for the model on the training and testing data will keep on decreasing. If it will learn for too long, the model will become more prone to overfitting due to the presence of noise and less useful details. Hence the performance of our model will decrease. In order to get a good fit, we will stop at a point just before where the error starts increasing. At this point, the model is said to have good skills in training datasets as well as our unseen testing dataset."
      ],
      "metadata": {
        "id": "s1g0yiVZ4erk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
        "\n",
        "Underfitting occurs when a model is too simple, which can be a result of a model needing more training time, more input features, or less regularization.\n",
        "\n",
        "Like overfitting, when a model is underfitted, it cannot establish the dominant trend within the data, resulting in training errors and poor performance of the model. If a model cannot generalize well to new data, then it cannot be leveraged for classification or prediction tasks. Generalization of a model to new data is ultimately what allows us to use machine learning algorithms every day to make predictions and classify data.\n",
        "\n",
        "High bias and low variance are good indicators of underfitting. Since this behavior can be seen while using the training dataset, underfitted models are usually easier to identify than overfitted ones.\n",
        "\n"
      ],
      "metadata": {
        "id": "kZFHM1Ky5L6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and\n",
        "variance, and how do they affect model performance?\n",
        "\n",
        "Bias Variance Tradeoff\n",
        "If the algorithm is too simple (hypothesis with linear equation) then it may be on high bias and low variance condition and thus is error-prone. If algorithms fit too complex (hypothesis with high degree equation) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as a Trade-off or Bias Variance Trade-off. This tradeoff in complexity is why there is a tradeoff between bias and variance. An algorithm can’t be more complex and less complex at the same time.\n"
      ],
      "metadata": {
        "id": "9xRvuk0P5l5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
        "How can you determine whether your model is overfitting or underfitting?\n",
        "\n",
        "Here are some common methods for detecting overfitting and underfitting in machine learning models:\n",
        "Test the model on new data\n",
        "Test the model on a new dataset that represents a variety of input data values and types. A high error rate in the test data indicates overfitting.\n",
        "Analyze learning curves\n",
        "Learning curves plot the model's performance over time on the training and validation datasets. Underfitting is indicated when the model performs poorly on the training data.\n",
        "Check validation metrics\n",
        "Validation metrics like accuracy and loss usually increase until they stagnate or start to decline when the model is overfitting.\n",
        "Check for high variance\n",
        "Overfitting can cause a model to be overly sensitive to noise in the training data, which leads to high variance in its predictions. This means the model can make very different predictions for similar inputs.\n",
        "To avoid underfitting, you can try increasing the number of features in the dataset, increasing model complexity, reducing noise in the data, or increasing the duration of training."
      ],
      "metadata": {
        "id": "NHer3CRX6TyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
        "and high variance models, and how do they differ in terms of their performance?\n",
        "\n",
        "Bias is simply defined as the inability of the model because of that there is some difference or error occurring between the model’s predicted value and the actual value. These differences between actual or expected values and the predicted values are known as error or bias error or error due to bias. Bias is a systematic error that occurs due to wrong assumptions in the machine learning process.\n",
        "Low Bias: Low bias value means fewer assumptions are taken to build the target function. In this case, the model will closely match the training dataset.\n",
        "High Bias: High bias value means more assumptions are taken to build the target function. In this case, the model will not match the training dataset closely.\n",
        "The high-bias model will not be able to capture the dataset trend. It is considered as the underfitting model which has a high error rate. It is due to a very simplified algorithm.\n",
        "\n",
        "For example, a linear regression model may have a high bias if the data has a non-linear relationship.\n",
        "\n",
        "What is Variance?\n",
        "Variance is the measure of spread in data from its mean position. In machine learning variance is the amount by which the performance of a predictive model changes when it is trained on different subsets of the training data. More specifically, variance is the variability of the model that how much it is sensitive to another subset of the training dataset. i.e. how much it can adjust on the new subset of the training dataset.\n",
        "\n",
        "Variance errors are either low or high-variance errors.\n",
        "\n",
        "Low variance: Low variance means that the model is less sensitive to changes in the training data and can produce consistent estimates of the target function with different subsets of data from the same distribution. This is the case of underfitting when the model fails to generalize on both training and test data.\n",
        "High variance: High variance means that the model is very sensitive to changes in the training data and can result in significant changes in the estimate of the target function when trained on different subsets of data from the same distribution. This is the case of overfitting when the model performs well on the training data but poorly on new, unseen test data. It fits the training data too closely that it fails on the new training dataset."
      ],
      "metadata": {
        "id": "f7e1biLx6j3Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
        "some common regularization techniques and how they work.\n",
        "\n",
        "Regularization Technique\n",
        "Regularization is a technique in machine learning that helps prevent from overfitting. It works by introducing penalties term or constraints on the model's parameters during training. These penalties term encourage the model to avoid extreme or overly complex parameter values. By doing so, regularization prevents the model from fitting the training data too closely, which is a common cause of overfitting. Instead, it promotes a balance between model complexity and performance, leading to better generalization on new, unseen data.\n",
        "\n",
        "How Regularization used to prevent overfitting\n",
        "By introducing the regularization term in loss function that act like a constrain function of the model's parameter. This function penalize certain parameter values in model, discouraging them from becoming too large or complex.\n",
        "Regularization introduces a trade-off between fitting the training data and keeping the model's parameters small. The strength of regularization is controlled by a hyperparameter, often denoted as lambda (λ). A higher λ value leads to stronger regularization and a simpler model.\n",
        "Regularization techniques help control the complexity of the model. They make the model more robust by constraining the parameter space. This results in smoother decision boundaries in the case of classification and smoother functions in regression, reducing the potential for overfitting.\n",
        "Regularization oppose overfitting by discouraging the model from fitting the training data too closely. It prevents parameters from taking extreme values, which might be necessary to fit the training data.\n",
        "Let's discuss about two common techniques that involve in regularization which can prevent model from overfitting\n",
        "\n",
        "L1 Regularization\n",
        "L2 Regularization\n",
        "\n",
        "L1 Regularization\n",
        "L1 regularization, also known as Lasso (Least Absolute Shrinkage and Selection Operator) regularization, is a statistical technique used in machine learning to avoid overfitting. It is used to add a penalty term to the model's loss function. This penalty term encourages the model to keep some of its coefficients exactly equal to zero, effectively performing feature selection. L1 regularization is employed to prevent overfitting, simplify the model, and enhance its generalization to new, unseen data. It is particularly useful when dealing with datasets containing many features, as it helps identify and focus on the most essential ones, disregarding less influential variables.\n",
        "\n",
        "L2 Regularization\n",
        "L2 regularization, often referred to as Ridge regularization, is a is a statistical technique used in machine learning to avoid overfitting. It involves adding a penalty term to the model's loss function, encouraging the model's coefficients to be small but not exactly zero. Unlike L1 regularization, which can lead to some coefficients becoming precisely zero, L2 regularization aims to keep all coefficients relatively small. This technique helps prevent overfitting, improves model generalization, and maintains a balance between bias and variance. L2 regularization is especially beneficial when dealing with datasets with numerous features, as it helps control the influence of each feature, contributing to more robust and stable model performance."
      ],
      "metadata": {
        "id": "5XxMd0bx9DxH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "thvUMq2U97wN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "AlE3BOWA5I5S"
      }
    }
  ]
}